{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f505c841-eba0-4a65-826b-529bd51d090d",
   "metadata": {},
   "source": [
    "# Optimizing data access and processing\n",
    "\n",
    "Over the multiple formats presented in the other tutorials, some were presented as cloud optimized. This notebook shows some methods to take advantage of those formats to optimize data access and processing. The methods presented are specifically optimized for cloud usage but can (and should most of the time) also be used with local files. The general idea is to limit the data access: either by loading only the required data or opening small fractions (chunks) of a whole dataset and iterate over all these chunks.\n",
    "\n",
    "\n",
    "## Data used\n",
    "This tutorial assumes the user ran the other notebooks before, or at least the corresponding ones, e.g. to run the *raster* section of this notebook, it is recommended to run the [raster formats tutorial](./raster_formats.ipynb) first. They give context on the data used and basic instructions on how to deal with the different types and formats. Moreover, the other tutorials download or generate sample data used as example, so make sure you at least ran the generation parts of the corresponding tutorials, or use your own data.\n",
    "\n",
    "## Raster data\n",
    "\n",
    "In the [raster formats tutorial](./raster_formats.ipynb), COGs (Cloud Optimized Geotiffs) were presented as the best cloud-oriented solution, as they are natively split and compressed in chunks and allow overviews. In this section, we present how to load and process a raster chunk by chunk. This is particularly useful when using large datasets which can't be loaded in memory (both on local or cloud architectures) or when accessing a specific sub area of a temporal series of images.\n",
    "\n",
    "### Using rasterio\n",
    "\n",
    "Reading chunks with rasterio is done using a window: a smaller section of the image, defined by an offset and a size on the x and y axes.\n",
    "\n",
    "\n",
    "A simple way to implement reading an image chunk by chunk is using chunks with the same width as the image (i.e. one or multiple lines) and iterating over them."
   ]
  },
  {
   "cell_type": "code",
   "id": "107f18b5-09e2-4a7e-a75e-a4d1517aaed7",
   "metadata": {},
   "source": [
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "cog_path = \"./sample_data/rasters/data/xt_SENTINEL2B_20180621-111349-432_L2A_T30TWT_D_V1-8_RVBPIR_cog.tif\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32b3e7bc-3684-4f7d-a715-8dd36b56982c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# read the entire raster (for comparison)\n",
    "with rasterio.open(cog_path) as src:\n",
    "    %time data = src.read(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "feaa5049-516f-4fa3-a26a-1a89845f3137",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# read chunk by chunk\n",
    "x_chunk_size = 1000  # number of columns per chunk\n",
    "y_chunk_size = 1000  # number of rows per chunk\n",
    "\n",
    "with rasterio.open(cog_path) as src:\n",
    "    width = src.width\n",
    "    height = src.height\n",
    "    for start_row in range(0, height, y_chunk_size):  # iterate over rows\n",
    "        end_row = min(start_row + y_chunk_size, height)\n",
    "        for start_col in range(0, width, x_chunk_size):  # iterate over columns\n",
    "            end_col = min(start_col + x_chunk_size, width)\n",
    "            window = Window(start_col, start_row, end_col - start_col, end_row - start_row)\n",
    "            %time data = src.read(1, window=window)  # read the 1st band\n",
    "            print(data.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "45fa458b-d339-41a5-a7e1-6dce970ad63d",
   "metadata": {},
   "source": [
    "But there is an easier and more optimized way of dealing with rasters by chunks. COGs have an intern chunk size used when writing them. It is easier and usually more efficient to use this chunk size instead, using `src.block_windows`. In this code `ji` is the current chunk's coordinates, relative to the chunk grid. The coordinates of the first pixel of the chunk are given by the widow's column and row offset."
   ]
  },
  {
   "cell_type": "code",
   "id": "2f666a3c-8244-48ee-a1b6-38ea477faf23",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "with rasterio.open(cog_path) as src:\n",
    "    for ji, window in src.block_windows(1):\n",
    "        print(f\"Chunk's coordinates: {ji}\")\n",
    "        print(window)\n",
    "        data = src.read(window=window)\n",
    "        print(data.shape, \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6a45b422-8dc1-4e65-9e56-0142f3f55a93",
   "metadata": {},
   "source": [
    "### Using rioxarray\n",
    "\n",
    "Rasterio opens raster as numpy arrays and processes them sequentially. Instead, it is possible to use `xioxarray`, a library to open rasters as xarrays:"
   ]
  },
  {
   "cell_type": "code",
   "id": "0bc8b66e-7747-4014-950f-f39b2af13d1e",
   "metadata": {},
   "source": [
    "import rioxarray as rxr\n",
    "\n",
    "xds = rxr.open_rasterio(cog_path, chunks=True)\n",
    "xds"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca1be867-40f9-4fda-8ed5-86af43b5ef7f",
   "metadata": {},
   "source": [
    "The `rioxarray` [official documentation](https://corteva.github.io/rioxarray/stable/examples/read-locks.html#Chunking) details the chunking process and how to optimize chunk management. This also allows the use of `dask` to process chunks in parallel. Once again, the official documentation has details on how to use dask with rioxarray: https://corteva.github.io/rioxarray/stable/examples/dask_read_write.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084eea72-4666-4ff0-997d-313b0edde803",
   "metadata": {},
   "source": [
    "## Vector data\n",
    "\n",
    "Vector data allows two types of optimizations:\n",
    "\n",
    "- streaming data chunk by chunk (similar to raster files)\n",
    "- filtering input data before loading it in memory\n",
    "\n",
    "These optimizations are not necesarily available for all vector formats. Geoparquet is the most suitable format for these uses: it can be easily streamed and filtered. \n",
    "\n",
    "### Streaming\n",
    "\n",
    "First, let's try reading data chunk by chunk. This can be easily done using `pyarrow`. To illustrate this example, we'll re-use the `landuse.parquet` file generated in the [vector formats tutorial](./vector_data_formats.ipynb) and count the number of forest polygons in each chunk (or batch)."
   ]
  },
  {
   "cell_type": "code",
   "id": "68033513-8c95-4cc5-9e8c-19271fa8ec17",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import pyarrow.parquet as pq\n",
    "file_path = \"./sample_data/vector/departement-31/landuse.parquet\"\n",
    "\n",
    "parquet_file = pq.ParquetFile(file_path)\n",
    "\n",
    "for n, batch in enumerate(parquet_file.iter_batches(batch_size=1000)):  # batch_size: number of rows to read\n",
    "    print(f\"Batch number {n}, n_rows: {batch.num_rows}\")\n",
    "    column=batch.column(\"type\").to_pandas()  # select the column 'type' and transform the column to a pandas series\n",
    "    n_forest = len(column[column == \"forest\"].index)  # count the occurrences of 'forest' \n",
    "    print(f\"{n_forest} occurences of 'forest'\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6b68bd0c-3d78-4543-ab30-d74873cea7ec",
   "metadata": {},
   "source": [
    "## Datacube\n",
    "\n",
    "Raster (and, in a less significant way vector) formats can be optimized for cloud computing, but may require some non-trivial work. However, most datacube formats are natively cloud-optimized (or can easily be optimized for cluod usage), like zarr. Let's reuse the `.zarr` file generated in the [data cube formats tutorial](./datacube_formats.ipynb). As explained in that tutorial, `.zarr` files are natively split into chunks to facilitate chunking mechanisms. Another optimization is lazy loading and computing. Lazy loading means that the data is not loaded into memory until needed; only the metadata is stored. This allows the user to define operations that, similarly, are not computed until required. The loading and computing steps are then optimized and executed at the end, improving ressource usage. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "id": "3a1cef2e-94d1-443f-bd10-2c1d8e9a5e0a",
   "metadata": {},
   "source": [
    "import hvplot.xarray\n",
    "import xarray as xr\n",
    "zarr_path = \"./sample_data/data_cube/example_from_xarray.zarr\"\n",
    "\n",
    "da = xr.open_dataset(zarr_path)[\"data\"]\n",
    "da.variable"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8900a6b5-29b9-4321-8660-813682977d20",
   "metadata": {},
   "source": [
    "The actual values of the array aren't loaded. One way of explicitly loading them into memory is using the `.load()` method:"
   ]
  },
  {
   "cell_type": "code",
   "id": "01c1c814-eb79-4ae5-8bbc-6871ead43224",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "da.load()\n",
    "da.variable"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e4c347b-cbd8-484f-8cc9-3b45f0bebcc1",
   "metadata": {},
   "source": [
    "Let's reopen it to reset it to lazy loaded and test lazy computing. For this, let's multiply the array by 10 and compute its mean over all time periods:"
   ]
  },
  {
   "cell_type": "code",
   "id": "a0030c03-f44e-4d5a-be66-c776189ee4c8",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "da = xr.open_dataset(zarr_path, chunks=\"auto\", engine=\"zarr\")[\"data\"]\n",
    "\n",
    "print(\"Lazy loaded array:\")  # lazy loaded\n",
    "print(da, \"\\n\\n\")\n",
    "\n",
    "scaled_data = da * 10\n",
    "mean_data = scaled_data.mean(dim=\"time\")  # lazy computed\n",
    "print(\"Lazy computed array:\")\n",
    "print(mean_data, \"\\n\\n\")\n",
    "\n",
    "\n",
    "result = mean_data.compute()  # load in memory and compute\n",
    "print(\"Loaded and computed:\")\n",
    "print(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d911053f-066b-4ecb-aea0-00076c2e165d",
   "metadata": {},
   "source": [
    "As explained in the rasters section, an efficient optimization method is to run chunks computation in parallel, using dask for example. And `xarray` uses dask to handle data arrays, making it easier to assemble these methods.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "There are many optimization techniques available for all the data types and formats presented, usable both on local and cloud architectures. It is important to consider which format and what processing methods can and should be used, depending on the architecture and ressources available, as well as the data volume. Generally, a modern and optimized code should be considered, but over a  small dataset, using parallel processing and chunking data may introduce unnecessary complexity and might be inefficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
